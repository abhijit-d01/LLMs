{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# RAG with Vertex AI: A Jupyter Notebook Guide\n",
    "\n",
    "This notebook demonstrates how to build a simple Retrieval-Augmented Generation (RAG) system using Google Cloud's Vertex AI.\n",
    "The RAG pipeline involves three main steps:\n",
    "1. **Data Pre-processing**: Extracting text from a PDF, chunking it into sentences, and generating vector embeddings.\n",
    "2. **Indexing**: Uploading the embeddings to a Vertex AI Vector Search (formerly Matching Engine) index for fast similarity searches.\n",
    "3. **Retrieval & Generation**: Given a user query, we retrieve relevant sentences from the index and use a Generative AI model (Gemini Pro) to formulate a grounded response.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting the RAG pipeline setup.\")\n",
    "#\n",
    "# ## 1. Setup and Initialization\n",
    "#\n",
    "# This section installs necessary Python packages, sets up the environment, and defines key variables for the project.\n",
    "#\n",
    "\n",
    "# Install required Python packages.\n",
    "# `pypdf2` is used for reading PDF files.\n",
    "# `google-cloud-aiplatform` is the Vertex AI SDK for interacting with Google Cloud's machine learning services.\n",
    "# `google-cloud-storage` is used for uploading files to Google Cloud Storage (GCS).\n",
    "!pip install pypdf2\n",
    "!pip install google-cloud-aiplatform\n",
    "!pip install google-cloud-storage\n",
    "\n",
    "# Import necessary libraries.\n",
    "# These libraries are essential for interacting with Google Cloud services and handling data.\n",
    "from google.cloud import storage\n",
    "from vertexai.language_models import TextEmbeddingModel\n",
    "from google.cloud import aiplatform\n",
    "import PyPDF2\n",
    "\n",
    "import re\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "import uuid\n",
    "\n",
    "# List the files in the current directory. This is useful for confirming\n",
    "# the presence of the PDF file you intend to use.\n",
    "%ls\n",
    "\n",
    "# Initialize some project variables.\n",
    "# Replace \"your_GCP_project_id\" with your actual Google Cloud Project ID.\n",
    "# The `location` is the Google Cloud region where your resources will be created.\n",
    "# project=\"your_GCP_project_id\"\n",
    "location=\"us-central1\"\n",
    "\n",
    "# Define file paths and resource names.\n",
    "# Ensure the `pdf_path` points to your document.\n",
    "# The `bucket_name` must be globally unique.\n",
    "pdf_path=\"stats.pdf\"\n",
    "bucket_name = \"stats-content2024\"\n",
    "embed_file_path = \"stats_embeddings.json\"\n",
    "sentence_file_path = \"stats_sentences.json\"\n",
    "index_name=\"stats_index\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#\n",
    "## 2. Helper Functions\n",
    "#\n",
    "This section defines a series of helper functions that perform the core tasks of the RAG pipeline.\n",
    "\n",
    "# Helper function to extract sentences from a PDF file.\n",
    "# It reads the PDF page by page, extracts the text, and splits it into a list of individual sentences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentences_from_pdf(pdf_path):\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        text = \"\"\n",
    "        for page in reader.pages:\n",
    "            if page.extract_text() is not None:\n",
    "                text += page.extract_text() + \" \"\n",
    "    sentences = [sentence.strip() for sentence in text.split('. ') if sentence.strip()]\n",
    "    return sentences\n",
    "def generate_text_embeddings(sentences) -> list:\n",
    "    # aiplatform.init(project=project,location=location) # This line is commented out but is needed for authentication\n",
    "    model = TextEmbeddingModel.from_pretrained(\"textembedding-gecko@001\")\n",
    "    embeddings = model.get_embeddings(sentences)\n",
    "    vectors = [embedding.values for embedding in embeddings]\n",
    "    return vectors\n",
    "def generate_and_save_embeddings(pdf_path, sentence_file_path, embed_file_path):\n",
    "    def clean_text(text):\n",
    "        cleaned_text = re.sub(r'\\u2022', '', text)  # Remove bullet points\n",
    "        cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()  # Remove extra whitespaces and strip\n",
    "        return cleaned_text\n",
    "    sentences = extract_sentences_from_pdf(pdf_path)\n",
    "    if sentences:\n",
    "        embeddings = generate_text_embeddings(sentences)\n",
    "        with open(embed_file_path, 'w') as embed_file, open(sentence_file_path, 'w') as sentence_file:\n",
    "            for sentence, embedding in zip(sentences, embeddings):\n",
    "                cleaned_sentence = clean_text(sentence)\n",
    "                id = str(uuid.uuid4())\n",
    "                embed_item = {\"id\": id, \"embedding\": embedding}\n",
    "                sentence_item = {\"id\": id, \"sentence\": cleaned_sentence}\n",
    "                json.dump(sentence_item, sentence_file)\n",
    "                sentence_file.write('\\n')\n",
    "                json.dump(embed_item, embed_file)\n",
    "                embed_file.write('\\n')\n",
    "def upload_file(bucket_name,file_path):\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.create_bucket(bucket_name,location=location)\n",
    "    blob = bucket.blob(file_path)\n",
    "    blob.upload_from_filename(file_path)\n",
    "def create_vector_index(bucket_name, index_name):\n",
    "    lakeside_index = aiplatform.MatchingEngineIndex.create_tree_ah_index(\n",
    "    display_name = index_name,\n",
    "    contents_delta_uri = \"gs://\"+bucket_name,\n",
    "    dimensions = 768,\n",
    "    approximate_neighbors_count = 10,\n",
    "    )\n",
    "    lakeside_index_endpoint = aiplatform.MatchingEngineIndexEndpoint.create(\n",
    "    display_name = index_name,\n",
    "    public_endpoint_enabled = True\n",
    "    )\n",
    "    lakeside_index_endpoint.deploy_index(\n",
    "    index = lakeside_index, deployed_index_id = index_name\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#\n",
    "## 3. Data Processing and Index Creation\n",
    "#\n",
    "This section executes the functions defined above to generate embeddings,\n",
    "upload them to GCS, and build the Vector Search index.\n",
    "\n",
    "**NOTE**: This process can take a significant amount of time, especially the index creation step.\n",
    "\n",
    "# Generate and save embeddings from the PDF.\n",
    "# This creates the `stats_sentences.json` and `stats_embeddings.json` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_and_save_embeddings(pdf_path,sentence_file_path,embed_file_path)\n",
    "# Upload the file containing the sentences to GCS.\n",
    "# This file will be used later to map embedding IDs back to the original sentences.\n",
    "upload_file(bucket_name,sentence_file_path)\n",
    "# Create the vector index and deploy it to a public endpoint.\n",
    "# The embeddings must be in a GCS bucket to be indexed.\n",
    "create_vector_index(bucket_name, index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#\n",
    "# 4. Retrieval and Generation\n",
    "#\n",
    "This section demonstrates how to use the created Vector Search index to retrieve relevant\n",
    "information and a Gemini model to generate a final, grounded answer.\n",
    "\n",
    "from vertexai.language_models import TextEmbeddingModel\n",
    "from google.cloud import aiplatform\n",
    "import vertexai\n",
    "from vertexai.preview.generative_models import GenerativeModel, Part\n",
    "import json\n",
    "import os\n",
    "# Initialize Vertex AI.\n",
    "# The project and location variables are used to set up the SDK environment.\n",
    "# project=”YOUR_GCP_PROJECT”\n",
    "location=\"us-central1\"\n",
    "# Define file path and index name.\n",
    "# The `sentence_file_path` is used to load the original sentences.\n",
    "# The `index_name` is the ID of the deployed Vector Search index.\n",
    "sentence_file_path = \"stats_sentences.json\"\n",
    "index_name=\"stats_index\" #Get this from the console or the previous step\n",
    "# aiplatform.init(project=project,location=location) # Uncomment and fill in project ID for a clean run.\n",
    "# vertexai.init() # Uncomment to initialize Vertex AI.\n",
    "# Initialize the generative model (Gemini Pro).\n",
    "model = GenerativeModel(\"gemini-pro\")\n",
    "# Connect to the deployed Vector Search index endpoint.\n",
    "# Replace the endpoint name with your actual index endpoint ID from the console.\n",
    "lakeside_index_ep = aiplatform.MatchingEngineIndexEndpoint(index_endpoint_name=\"1376179539650019328\")\n",
    "# Function to generate text embeddings for the user query.\n",
    "# This uses the same `TextEmbeddingModel` as the indexing step to ensure the\n",
    "# query embedding is in the same vector space.\n",
    "def generate_text_embeddings(sentences) -> list:\n",
    "    model = TextEmbeddingModel.from_pretrained(\"textembedding-gecko@001\")\n",
    "    embeddings = model.get_embeddings(sentences)\n",
    "    vectors = [embedding.values for embedding in embeddings]\n",
    "    return vectors\n",
    "# Function to retrieve the original sentences from the loaded data based on their IDs.\n",
    "def generate_context(ids,data):\n",
    "    concatenated_names = ''\n",
    "    for id in ids:\n",
    "        for entry in data:\n",
    "            if entry['id'] == id:\n",
    "                concatenated_names += entry['sentence'] + \"\\n\"\n",
    "    return concatenated_names.strip()\n",
    "# Function to load the sentence data from a JSON file.\n",
    "def load_file(sentence_file_path):\n",
    "    data = []\n",
    "    with open(sentence_file_path,'r') as f:\n",
    "        for line in f:\n",
    "            entry = json.loads(line)\n",
    "            data.append(entry)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#\n",
    "## 5. Query and Grounded Response\n",
    "#\n",
    "This section executes the retrieval and generation steps to answer a user query.\n",
    "# Load the sentence data into memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=load_file(sentence_file_path)\n",
    "data\n",
    "# Define the user query.\n",
    "query=[\"what is correlation?\"]\n",
    "# Generate embeddings for the user query.\n",
    "qry_emb=generate_text_embeddings(query)\n",
    "# qry_emb\n",
    "# Perform a similarity search on the Vector Search index.\n",
    "# It retrieves the 10 nearest neighbors (most relevant sentences) to the query.\n",
    "response = lakeside_index_ep.find_neighbors(\n",
    "    deployed_index_id = index_name,\n",
    "    queries = [qry_emb[0]],\n",
    "    num_neighbors = 10\n",
    ")\n",
    "# Extract the IDs of the top-k matching sentences.\n",
    "matching_ids = [neighbor.id for sublist in response for neighbor in sublist]\n",
    "# Use the matching IDs to retrieve the full, original sentences.\n",
    "context = generate_context(matching_ids,data)\n",
    "# Create the final prompt by combining the retrieved context and the user's query.\n",
    "# This is a key step in RAG, as it grounds the LLM's response in the provided context.\n",
    "prompt=f\"Based on the context delimited in backticks, answer the query. ```{context}``` {query}\"\n",
    "# Start a chat session with Gemini and send the grounded prompt.\n",
    "# The model will use the provided context to generate an answer.\n",
    "chat = model.start_chat(history=[])\n",
    "response = chat.send_message(prompt)\n",
    "# Print the final, grounded response from the model.\n",
    "print(response.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
